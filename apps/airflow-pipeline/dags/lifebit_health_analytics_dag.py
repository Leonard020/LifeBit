"""
LifeBit ê±´ê°• ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ DAG

ì´ DAGëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ê±´ê°• ë°ì´í„° ì¶”ì¶œ (Extract)
2. ë°ì´í„° ë³€í™˜ ë° ì •ì œ (Transform)
3. ë¶„ì„ ê²°ê³¼ ì €ì¥ (Load)
4. AI ì¶”ì²œ ìƒì„±
5. ì•Œë¦¼ ë°œì†¡

ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ ìµœì†Œí•œì˜ ë¦¬ì†ŒìŠ¤ë¡œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'lifebit-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,  # ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•´ ë™ì‹œ ì‹¤í–‰ ì œí•œ
}

# DAG ì •ì˜
dag = DAG(
    'lifebit_health_analytics_pipeline',
    default_args=default_args,
    description='LifeBit ê±´ê°• ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ (MVP)',
    schedule_interval='@daily',  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,  # ê³¼ê±° ë°ì´í„° ë°±í•„ ë¹„í™œì„±í™”
    max_active_tasks=2,  # ë™ì‹œ ì‹¤í–‰ íƒœìŠ¤í¬ ìˆ˜ ì œí•œ
    tags=['lifebit', 'health', 'analytics', 'mvp']
)

# Python í•¨ìˆ˜ë“¤
def extract_health_data(**context):
    """
    LifeBit ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê±´ê°• ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    import psycopg2
    import pandas as pd
    import os
    from datetime import datetime, timedelta
    
    logging.info("ğŸ” ê±´ê°• ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    
    # PostgreSQL ì—°ê²° ì •ë³´
    db_config = {
        'host': os.getenv('LIFEBIT_DB_HOST', 'host.docker.internal'),
        'port': int(os.getenv('LIFEBIT_DB_PORT', 5432)),
        'user': os.getenv('LIFEBIT_DB_USER', 'lifebit_user'),
        'password': os.getenv('LIFEBIT_DB_PASSWORD', 'lifebit_password'),
        'database': os.getenv('LIFEBIT_DB_NAME', 'lifebit_db')
    }
    
    try:
        # ì–´ì œ ë‚ ì§œ ê³„ì‚°
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # PostgreSQL ì—°ê²°
        connection = psycopg2.connect(
            host=db_config['host'],
            port=db_config['port'],
            user=db_config['user'],
            password=db_config['password'],
            database=db_config['database']
        )
        
        # ê±´ê°• ê¸°ë¡ ë°ì´í„° ì¶”ì¶œ
        health_query = f"""
        SELECT user_id, record_date, weight, height, bmi, created_at
        FROM health_records 
        WHERE record_date >= '{yesterday}'
        ORDER BY record_date DESC, created_at DESC
        """
        
        health_data = pd.read_sql(health_query, connection)
        logging.info(f"âœ… ê±´ê°• ê¸°ë¡ {len(health_data)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        
        # ìš´ë™ ì„¸ì…˜ ë°ì´í„° ì¶”ì¶œ
        exercise_query = f"""
        SELECT es.user_id, es.exercise_date, ec.name as exercise_name, 
               es.duration_minutes, es.calories_burned, es.notes, es.created_at
        FROM exercise_sessions es
        LEFT JOIN exercise_catalog ec ON es.exercise_catalog_id = ec.exercise_catalog_id
        WHERE es.exercise_date >= '{yesterday}'
        ORDER BY es.exercise_date DESC, es.created_at DESC
        """
        
        exercise_data = pd.read_sql(exercise_query, connection)
        logging.info(f"âœ… ìš´ë™ ê¸°ë¡ {len(exercise_data)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        
        # ì‹ë‹¨ ë¡œê·¸ ë°ì´í„° ì¶”ì¶œ
        meal_query = f"""
        SELECT ml.user_id, ml.log_date, fi.name as food_name, 
               ml.quantity, fi.calories, fi.carbs, fi.protein, fi.fat, 
               ml.meal_time, ml.created_at
        FROM meal_logs ml
        LEFT JOIN food_items fi ON ml.food_item_id = fi.food_item_id
        WHERE ml.log_date >= '{yesterday}'
        ORDER BY ml.log_date DESC, ml.created_at DESC
        """
        
        meal_data = pd.read_sql(meal_query, connection)
        logging.info(f"âœ… ì‹ë‹¨ ê¸°ë¡ {len(meal_data)}ê±´ ì¶”ì¶œ ì™„ë£Œ")
        
        connection.close()
        
        # Timestamp ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
        def convert_timestamps(df):
            """DataFrameì˜ Timestamp ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
            df_copy = df.copy()
            for col in df_copy.columns:
                if df_copy[col].dtype == 'datetime64[ns]' or 'datetime' in str(df_copy[col].dtype):
                    df_copy[col] = df_copy[col].astype(str)
            return df_copy
        
        health_data_clean = convert_timestamps(health_data)
        exercise_data_clean = convert_timestamps(exercise_data)
        meal_data_clean = convert_timestamps(meal_data)
        
        # XComì„ í†µí•´ ë‹¤ìŒ íƒœìŠ¤í¬ë¡œ ë°ì´í„° ì „ë‹¬
        return {
            'health_records': health_data_clean.to_dict('records'),
            'exercise_sessions': exercise_data_clean.to_dict('records'),
            'meal_logs': meal_data_clean.to_dict('records'),
            'extraction_date': yesterday,
            'total_records': len(health_data) + len(exercise_data) + len(meal_data)
        }
        
    except Exception as e:
        logging.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        raise


def transform_and_analyze_data(**context):
    """
    ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime
    
    logging.info("ğŸ”„ ë°ì´í„° ë³€í™˜ ë° ë¶„ì„ ì‹œì‘")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    extracted_data = ti.xcom_pull(task_ids='extract_health_data')
    
    if not extracted_data:
        logging.warning("âš ï¸ ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {"status": "no_data", "analysis_results": {}}
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
        health_df = pd.DataFrame(extracted_data['health_records'])
        exercise_df = pd.DataFrame(extracted_data['exercise_sessions'])
        meal_df = pd.DataFrame(extracted_data['meal_logs'])
        
        analysis_results = {}
        
        # ê±´ê°• ì§€í‘œ ë¶„ì„
        if not health_df.empty:
            analysis_results['health_analysis'] = {
                'average_weight': float(health_df['weight'].mean()) if 'weight' in health_df.columns else 0,
                'average_bmi': float(health_df['bmi'].mean()) if 'bmi' in health_df.columns else 0,
                'users_count': int(health_df['user_id'].nunique()),
                'records_count': len(health_df)
            }
        
        # ìš´ë™ ë¶„ì„
        if not exercise_df.empty:
            analysis_results['exercise_analysis'] = {
                'total_exercise_time': int(exercise_df['duration_minutes'].sum()) if 'duration_minutes' in exercise_df.columns else 0,
                'total_calories_burned': int(exercise_df['calories_burned'].sum()) if 'calories_burned' in exercise_df.columns else 0,
                'active_users': int(exercise_df['user_id'].nunique()),
                'exercise_sessions': len(exercise_df)
            }
        
        # ì‹ë‹¨ ë¶„ì„
        if not meal_df.empty:
            analysis_results['nutrition_analysis'] = {
                'total_calories_consumed': int(meal_df['calories'].sum()) if 'calories' in meal_df.columns else 0,
                'average_protein': float(meal_df['protein'].mean()) if 'protein' in meal_df.columns else 0,
                'average_carbs': float(meal_df['carbs'].mean()) if 'carbs' in meal_df.columns else 0,
                'meal_logs_count': len(meal_df)
            }
        
        # ì¢…í•© ë¶„ì„
        analysis_results['summary'] = {
            'analysis_date': extracted_data['extraction_date'],
            'total_records_processed': extracted_data['total_records'],
            'processing_timestamp': datetime.now().isoformat()
        }
        
        logging.info(f"âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ: {analysis_results}")
        
        return {
            'status': 'success',
            'analysis_results': analysis_results
        }
        
    except Exception as e:
        logging.error(f"âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        raise


def generate_ai_recommendations(**context):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    import requests
    import os
    
    logging.info("ğŸ¤– AI ì¶”ì²œ ìƒì„± ì‹œì‘")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    transform_result = ti.xcom_pull(task_ids='transform_and_analyze_data')
    
    if not transform_result or transform_result['status'] != 'success':
        logging.warning("âš ï¸ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ AI ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {"status": "no_analysis_data"}
    
    try:
        analysis = transform_result['analysis_results']
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¶”ì²œ ìƒì„± (MVPìš©)
        recommendations = []
        
        # ìš´ë™ ì¶”ì²œ
        if 'exercise_analysis' in analysis:
            exercise_data = analysis['exercise_analysis']
            avg_exercise_time = exercise_data.get('total_exercise_time', 0) / max(exercise_data.get('active_users', 1), 1)
            
            if avg_exercise_time < 30:
                recommendations.append({
                    'type': 'exercise',
                    'priority': 'high',
                    'message': 'ì¼ì¼ ìš´ë™ ì‹œê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 30ë¶„ ì´ìƒì˜ ìš´ë™ì„ ê¶Œì¥í•©ë‹ˆë‹¤.',
                    'suggestion': 'ê°€ë²¼ìš´ ì‚°ì±…ì´ë‚˜ í™ˆíŠ¸ë ˆì´ë‹ë¶€í„° ì‹œì‘í•´ë³´ì„¸ìš”.'
                })
        
        # ì˜ì–‘ ì¶”ì²œ
        if 'nutrition_analysis' in analysis:
            nutrition_data = analysis['nutrition_analysis']
            avg_protein = nutrition_data.get('average_protein', 0)
            
            if avg_protein < 50:
                recommendations.append({
                    'type': 'nutrition',
                    'priority': 'medium',
                    'message': 'ë‹¨ë°±ì§ˆ ì„­ì·¨ëŸ‰ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
                    'suggestion': 'ë‹­ê°€ìŠ´ì‚´, ê³„ë€, ë‘ë¶€ ë“± ë‹¨ë°±ì§ˆì´ í’ë¶€í•œ ìŒì‹ì„ ë“œì„¸ìš”.'
                })
        
        # ì¼ë°˜ ê±´ê°• ì¶”ì²œ
        recommendations.append({
            'type': 'general',
            'priority': 'low',
            'message': 'ê¾¸ì¤€í•œ ê±´ê°• ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.',
            'suggestion': 'ê·œì¹™ì ì¸ ìƒí™œ íŒ¨í„´ì„ ìœ ì§€í•˜ê³  ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ì„¸ìš”.'
        })
        
        logging.info(f"âœ… AI ì¶”ì²œ {len(recommendations)}ê°œ ìƒì„± ì™„ë£Œ")
        
        return {
            'status': 'success',
            'recommendations': recommendations,
            'recommendation_count': len(recommendations)
        }
        
    except Exception as e:
        logging.error(f"âŒ AI ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise


def send_summary_notification(**context):
    """
    ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ë¡œê·¸ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. (MVPìš© ê°„ë‹¨ ì•Œë¦¼)
    """
    logging.info("ğŸ“¢ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì•Œë¦¼")
    
    # ì´ì „ íƒœìŠ¤í¬ë“¤ì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    ti = context['ti']
    analysis_result = ti.xcom_pull(task_ids='transform_and_analyze_data')
    ai_result = ti.xcom_pull(task_ids='generate_ai_recommendations')
    
    try:
        summary = {
            'pipeline_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_analysis': analysis_result.get('analysis_results', {}) if analysis_result else {},
            'ai_recommendations': ai_result.get('recommendations', []) if ai_result else [],
            'status': 'completed'
        }
        
        logging.info("=" * 50)
        logging.info("ğŸ“Š LifeBit ê±´ê°• ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
        logging.info("=" * 50)
        
        if analysis_result and analysis_result.get('status') == 'success':
            analysis = analysis_result['analysis_results']
            
            if 'health_analysis' in analysis:
                health = analysis['health_analysis']
                logging.info(f"ğŸ‘¥ ê±´ê°• ê¸°ë¡: {health.get('users_count', 0)}ëª…, {health.get('records_count', 0)}ê±´")
                logging.info(f"âš–ï¸ í‰ê·  ì²´ì¤‘: {health.get('average_weight', 0):.1f}kg")
                logging.info(f"ğŸ“ í‰ê·  BMI: {health.get('average_bmi', 0):.1f}")
            
            if 'exercise_analysis' in analysis:
                exercise = analysis['exercise_analysis']
                logging.info(f"ğŸ’ª ìš´ë™ ì„¸ì…˜: {exercise.get('exercise_sessions', 0)}ê±´")
                logging.info(f"â° ì´ ìš´ë™ ì‹œê°„: {exercise.get('total_exercise_time', 0)}ë¶„")
                logging.info(f"ğŸ”¥ ì´ ì†Œëª¨ ì¹¼ë¡œë¦¬: {exercise.get('total_calories_burned', 0)}kcal")
            
            if 'nutrition_analysis' in analysis:
                nutrition = analysis['nutrition_analysis']
                logging.info(f"ğŸ½ï¸ ì‹ë‹¨ ê¸°ë¡: {nutrition.get('meal_logs_count', 0)}ê±´")
                logging.info(f"ğŸ¥— ì´ ì„­ì·¨ ì¹¼ë¡œë¦¬: {nutrition.get('total_calories_consumed', 0)}kcal")
        
        if ai_result and ai_result.get('status') == 'success':
            recommendations = ai_result['recommendations']
            logging.info(f"ğŸ¤– AI ì¶”ì²œ: {len(recommendations)}ê°œ ìƒì„±")
            
            for i, rec in enumerate(recommendations, 1):
                logging.info(f"   {i}. [{rec['priority'].upper()}] {rec['message']}")
        
        logging.info("=" * 50)
        
        return summary
        
    except Exception as e:
        logging.error(f"âŒ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {str(e)}")
        raise


# íƒœìŠ¤í¬ ì •ì˜
start_task = DummyOperator(
    task_id='start_pipeline',
    dag=dag
)

# ë°ì´í„° ì¶”ì¶œ íƒœìŠ¤í¬
extract_task = PythonOperator(
    task_id='extract_health_data',
    python_callable=extract_health_data,
    dag=dag
)

# ë°ì´í„° ë³€í™˜ ë° ë¶„ì„ íƒœìŠ¤í¬
transform_task = PythonOperator(
    task_id='transform_and_analyze_data',
    python_callable=transform_and_analyze_data,
    dag=dag
)

# AI ì¶”ì²œ ìƒì„± íƒœìŠ¤í¬
ai_task = PythonOperator(
    task_id='generate_ai_recommendations',
    python_callable=generate_ai_recommendations,
    dag=dag
)

# ì•Œë¦¼ ë°œì†¡ íƒœìŠ¤í¬
notification_task = PythonOperator(
    task_id='send_summary_notification',
    python_callable=send_summary_notification,
    dag=dag
)

# ì™„ë£Œ íƒœìŠ¤í¬
end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
start_task >> extract_task >> transform_task >> ai_task >> notification_task >> end_task
